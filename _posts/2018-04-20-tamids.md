---
title: "TAMIDS Data Science Competition 2018"
date: 2018-04-20
tags: [Data Analytics]
header:
  image: "/images/tamids/taxiheader.jpg"
excerpt: "1st place entry in the 2018 Texas A&M Data Science Competition.
The project focuses on the public Chicago taxi ridership dataset from 2013 through
2017."
toc: true
toc_label: Table of Contents
---

Chicago today is a city of about 2.7 million people filled with tourist locations, business skyscrapers, and nightlife hotspots. The combination of high density destinations and strong tourist industry create a huge market for taxis, and Chicago’s taxi drivers have historically had a higher salary than those in most other cities, with a median of about $36,000 a year, with a few hotspots being moneymakers for these drivers. In recent years, however, the rideshare industry has changed the landscape of the transit markets in Chicago.  

Two competing ridesharing companies, Uber and Lyft, have seen rapid growth in recent years across the United States. While it’s possible that a portion of their business comes from growth in the ridesharing/taxi market itself, the rise of these ridesharing companies (and other smaller companies) has resulted in the decline of the taxi industry in many areas. Chicago’s taxi industry
in particular has taken a hit, as Uber and Lyft grew in the city with no major restrictions being placed on the ridesharing companies until the summer of 2016. The data used in this competition contains four years of training data and a segment of 2017’s data, which allow us to both explore Chicago’s taxi industry as a whole and fit predictive models on variables of interest. We are interested in what changes individual taxi drivers in Chicago experience, and forecasting future changes to these taxi drivers is of huge interest to the city of Chicago, especially as they consider adding harsher legislation to counter ridesharing companies. Forecasting the median daily fares will give us an idea of both how much business a typical taxi driver will see in a day as well as how much they will earn. In doing so, we aim to predict the future prospects of a typical Chicago taxi driver.

# Exploratory Data Analysis
In order to efficiently perform exploratory data analysis, we used a subset containing 1% of the data for each of the provided training datasets so that our subset’s yearly proportions were the same as those of the full data’s. Hourly, daily, monthly and yearly trends were compared over repeated subsets to demonstrate that the trends in each subset were representative of the full data’s trends. It should be noted that the EDA plots and summary statistics examined in this section of the report are all run on the same 1% subset.  

To create the visualizations used in Section 1 of the appendix, we created our subsets from the data with Perl, then loaded those subsets into R. We then processed the data to match up column names and to bind the data for 2013-2016 together. We used basic R commands like summary, median, and plot(x,y) for general exploration, and used functions from the ggplot package to produce the tables shown in the appendix. Segments of our cleaned up code are given at the bottom of the appendix.  

While exploring the data, we discovered some large scale trends. The number of rows (trip counts) in each dataset was noticeably different, with a decline from 2014 to 2016. Fares, trip lengths, and trip distances declined as well, though less dramatically than counts. Tips and tolls do not follow yearly trends. Each subset contained some extreme outliers, with some distances or fares being extraordinarily high relative to the other. We found that taking the medians of these variables with outliers by day/week/month erased the effects of the outliers, and that it was not necessary for EDA to exclude these outliers.  

Before taking a deep dive into how individual taxi drivers have been affected by changes to the industry, it is important to study the underlying seasonal trends and year to year changes of the industry as a whole. To effectively do so, we looked at median trip duration and the total number of trips respectively by hour of day, day of week, and month of year. Median trip duration was chosen as it is highly related to trip fare, our primary variable of interest for analysis. Trip count was chosen to show both the seasonality of taxi demand and the changes in demand from 2013 to 2016.  

We will begin by studying median trip duration over the years.  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/MedianDurationMonth.jpeg" alt="median trip duration by month">  

With a slight exception in 2013 each year has very similar trends, with a peak from May to June followed by a steady decline until another peak in October. 2016 appears to be smaller in magnitude than the previous years, suggesting that trip duration might be declining.  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/MedianDurationDay.jpeg" alt="median trip duration by day">  

This figure shows us that the proportions of trip duration by day of the week remain constant across the years, with Friday and Saturday peaking before a sharp decline to Sunday.  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/MedianDurationHour.jpeg" alt="median trip duration by hour">  

Like in the previous figure, the trends in the above figure appear constant over the years. Trip duration bottoms out at 5am and, unsurprisingly, peaks sharply from approximately 5-8 PM, when many people get out of work and/or go to dinner.  

Next, we will take a close look at total trip counts.  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/TripCountMonth.jpeg" alt="trip count by month">  

This figure clearly shows the difference from year to year, with a rise from 2013 to 2014 and a sharp drop from 2015 to 2016. The interval March-June is the highest overall, though 2013 and 2016 have more of a general upward and downward trend, respectively. Like with time duration, there is a peak each year in October.  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/TripCountDay.jpeg" alt="trip count by day">  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/TripCountHour.jpeg" alt="trip count by hour">  

The trip count for Day and Hour have trends identical to those of Median Trip Duration for Day and Hour respectively, while the trends are slightly different on the monthly scale.  

By studying the trends of counts and median duration, we are beginning to paint a picture of Chicago taxi trips. A variable of interest that has not yet been examined, location, can provide us with a more literal picture of these trips. Since taxi trip counts showed the clearest trend over years while having very similar trends by day and hour, we will take a closer look at the density of taxi trips by location and look for key points by multiple levels. Specifically, we will look at the counts at each coordinate (to 4 decimals) by hour of day, day of week, month of year, and across years. In addition to visualizing the data from the training data, we wanted to see how useful some basic machine learning models could be in predicting future counts at a given coordinate.  

Using the 1% subset, we tried out two ML techniques that we believed could be successful in this endeavor: Random forests and K-nearest neighbor algorithms. After performing initial tests on the 1% subset for comparisons, we found that random forests were both significantly faster and produced much lower errors and error rates.

# Predicting Pickup Density using Random Forest
To predict taxi pickup density, we count the number of taxi pickups in the city of Chicago at a unique longitude and latitude coordinate, year, month, day of the month, and hour of day. We then do some investigations to see if we can find any explanations for any trends we find.  

After running our model, we find that location (latitude and longitude) was by far the most important feature in predicting number of pickups. We also saw that while there were definitely some trends in number of pickups at all levels of time, the most distinguishable trends were found at different hours of the day. Because we will turn our focus on pickup density at the hour level.  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/HourDensity.gif" alt="pickup density">  

We can see that the pickup density by the hour follows the trend from our EDA histograms. The busiest time of the day is 7:00PM and the least busiest is around 4:00AM and 5:00AM. We can also see that our predicted pickup density is pretty accurate and the change in density over time is representative of the actual number of pickups. Predicting taxi pickups is only the first step, we now want to investigate any reasons that would explain why the pickups are concentrated in specific. To do so, let's take closer look at one of the pickup hotspots.    

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/RFmap.jpg" alt="map overlay">  

Here is a map of Chicago at 7:00 the busiest time of the day we found, where the pickups are most concentrated. The blue “M” icons represent metro stations for the Chicago metro subways and the pizza icons represent restaurants that serve Chicago’s famous deep dish pizza. Notable tourist attractions are also represented on the map with their respective icons. From the map, we can see that the two points with the most pickups (73,274 and 40,053) are the closest in proximity with the two hottest Chicago tourist attractions: Willis Tower and the Bean. These high density points can also be found in near proximity to the Chicago metro stations. However, we believe that the biggest influencing factors for taxi pickups is the proximity to the metro stations. With the one of the largest metro stations systems in the U.S, only second to New York City, and Chicago being one of the biggest cities for business, it makes sense why that dense pickup locations are all in close proximity to a metro station.  

Performing prediction on the 2017 test data, we get only a 43.27% accuracy rate, not so good. However, for our particular study, a low prediction accuracy does not necessary mean that our model is not good. This accuracy score only accounts for the number of pickups that our model has predicted exactly and does not factor in the proximity of our prediction, and considering the scale of the data we are working with, the proximity to the exact solution is pretty important to assess. For example, if a specific location had 1,000 true pickups, but our model predicted that same location would have 998 pickups. That prediction is not exactly correct but since we are working with a dataset containing several million pickups, a prediction that is off by only a factor of 2 is fairly accurate. So to take a further inspection on how good our model is, we must take into account how far our predictions are off from the true values and we can do that by calculating the root mean square error. Our calculated RMSE for our model was 0.141, but that is in log space so we must transform back. Doing so, we obtain a value of 1.382. That means that the other 56% of our predictions are only a factor of 1.382 or less away from the true value. Now remember we are working with a dataset containing several million pickups, so our model being only a factor of 1.382 pickups away from the true pickup values means that model is predicting pickup values very close to the actual number of pickups. Knowing this, now our 43% prediction accuracy is actually quite considerably good.  

# Modeling and Analysis
After conducting our initial exploratory data analysis on the 1% subsetted data, we conclude that there was a strong change in median fare over time in a taxi driver’s median daily fare and time variables such as year, month, day, and hour. We choose to analyze the data with respect to daily values as opposed to hourly, weekly, or yearly time intervals. This is simply because hourly values contain too many sources of variation (seasonal effects), weekly values do not provide enough information as to determine the effects of weekends vs weekdays, and yearly values do not provide nearly enough information to produce any significant analysis.  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_1.png" alt="figure 3.1">  

The above demonstrates the daily median taxi fare from 2013 to 2016. From this plot, it is clear that there exists a correlation between the two variables as well as some form of seasonal effect which dictates the pattern of the data at certain parts of the year. This further supports our claim for the need of some form of a time series model.  

The next step in analyzing this time dependent data, is to find out if the general trends shown by the 1% subset of the median daily fare date resemble that of the full data. As it turns out, the general trend between days and median fares were quite similar, however, the plots produced seem to contain a lot of noise which made it difficult to see a clear trend. Therefore, to remedy this, we compute both weekly and monthly moving averages in order to minimize the noise and get a clearer picture of the overall trend direction. A moving average is the mean of the data during several consecutive periods and is computed by averaging a series of past and future numerical values from a time series data. For instance, we calculate the weekly moving average by taking a series of 7 sequential numbers (3 prior observations, the current observation at time t, and the following 3 observations) to create a single point. Likewise, we calculate the monthly moving average by taking a series of 30 sequential numbers and averaging them to create a single point. This concept is shown below:  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_2.png" alt="figure 3.2">  

It is important to note that the median taxi fare over time is represented as red while the weekly and monthly moving averages are represented as blue and green respectively. Also, it is clear from this plot that the raw data contains high levels of variation and fluctuates over time making it difficult to detect a general trend while the monthly moving average seems to be too robust and and doesn’t respond well towards seasonal effects. With that being said, the weekly moving average seems to be the best method for analyzing trends in the data as it is not too robust towards variation and displays seasonal effects while not being overly sensitive to them at the same time.  

Since the weekly moving average proved to be the more efficient form of measurement, we decompose the data with respect to the weekly moving average in order to analyze the seasonal effects as well as the general trend of the data with minimal noise.  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_3.png" alt="figure 3.3">  

The above shows the raw data being plotted in the top row, the seasonal effects of the data in the second row, the general trend throughout the years in the third row, and the residuals of the data with respect to time on the fourth row. However, in order to conduct a valid time series analysis, we must remove the seasonality effects from the data.  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_4.png" alt="figure 3.4">  

This shows the weekly moving average with the seasonality component subtracted from the original series, hence a deseasonal moving average. We perform an Augmented Dickey-Fuller test on the deseasonal data in order to determine the stationary status of the weekly moving. Since the test provided a p-value of 0.01, we reject the null hypothesis and conclude that the data is indeed stationary. However, looking back at our deseasonal weekly moving average plot, we see that the data has unequal variances with no stable mean. This means that we are unable to extrapolate any meaningful information by conducting time series analysis. To remedy this, we differentiate the deaseasonal weekly moving average data in order to adjust for the unequal variances and unstable mean. Then we conduct another Augmented Dickey-Fuller test to ensure we have stationary data. The plots below demonstrates the data after differentiating and shows a clear mean throughout the data centered at around y = 0 with a variance that is about the same throughout.  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_51.png" alt="figure 3.51">
<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_52.png" alt="figure 3.52">  

Autocorrelation plot:  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_6.png" alt="figure 3.6">  

Partial correlation plot:  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_7.png" alt="figure 3.7">  

From our autocorrelation plot we see that there are significant auto correlations at lags 1, 2, 3, and 4 while the partial correlation plots show significant spikes at lags 1 and 8. This indicates that we need to test models that incorporat autoregressive (AR) factors or moving averages (MA). In other words, we need to form an ARIMA model with components of 1,2,3,4 or 7. Since our autocorrelation plot inverts right after the 4th lag, we decide to use p = 3 as our autocorrelation coefficient. Also, since our partial autocorrelation plot seems to have a repeating spike at every 8th lag, which could be explained by a weekly seasonal effect, it suggests that we must use q = 8 in our model. The last component of the ARIMA model was determined earlier on in the analysis when we determined the stationarity of the data after differencing. Since we achieved equal mean and variances throughout the data by differencing only once, we conclude that the final ARIMA parameter d is equal to 1.  

After obtaining our own ARIMA order values (3,1,8) by hand, we want to compare our model against the optimal model obtained by the function auto.arima located in the forecast package in R. The function auto.arima finds the optimal order values by minimizing and comparing the AIC values of all possible order coefficients. As one could imagine, this process takes a very long time to compute, so therefore we opt to use a stepwise auto.arima parameter in order to cut down run time dramatically. After running the auto.arima function, the optimal order coefficients produced are (4,1,4), which differs greatly compared to our handpicked (3,1,8).
In order to test the importance of the seasonal coefficients, we take our handpicked “naive” model and make an identical copy to which we turn off seasonality thus ultimately resulting in 3 different ARIMA models. The first ARIMA model is our handpicked naive model with seasonality turned off. It is shown below:  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_8.png" alt="figure 3.8">  

It is important to notice that the data provided for 2013- 2016 is shaded in as black while the forecasted predictions are displayed in dark blue. The forecasted data spikes up to about $150 and then plateaus for the duration of 2017. This is due to the lack of seasonality coefficients. It is also important to recognize that there are two shaded regions produced in the forecast of all three models. These shaded regions are prediction intervals with dark blue pertaining to an 80% prediction interval and light blue to a 95% prediction interval. Note that the prediction intervals get wider as time increases. This is natural since things become harder to predict, and thus more uncertain, as time increases. The second ARIMA model is again our handpicked naive model, however, this time seasonality is turned on. It can be seen below:  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_9.png" alt="figure 3.9">  

Since seasonality is turned on for this model, we are able to see the forecast adapt to seasonal trends and better forecast what the median daily fare in 2017 would be like for the average taxi driver in Chicago. Lastly, we have our model with optimized AIC chosen order coefficients selected by auto.arima:  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_10.png" alt="figure 3.10">  

Again, this model has seasonality activated and thus is able to forecast future values while taking into account seasonal trends. It is important to note that these prediction intervals for this model are much larger than those in our second ARIMA model with time seasonality turned on.  

The last thing we do to ensure that our ARIMA model is the correct way of modeling this data, is to produce an entirely different time series model, namely a time series linear model. The two major differences between an ARIMA model and a time series linear model are that the time series linear model is not produced from stationary data and does not take into account autocorrelation.  

After having these 4 models to compare to, we took a look at several measures of accuracy to assess model fit.  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_11.jpg" alt="figure 3.11">  

By taking a look at the root mean squared error (RMSE), we can clearly see that the time series linear model has the lowest accuracy since it has an RMSE of 40.25 while the other three models have an RMSE ranging between 1.86 to 2.47. As stated before, the time series linear model does not take into account autocorrelation, which is further demonstrated above where ACF1 is marked as NA for the time series linear model. All in all, it seems as if ARIMA is the way to go in terms of modeling this time series data since all of the measures of accuracy were around the same range for the ARIMA models while the time series linear model is always higher.  

The next table shows which of the ARIMA models serve as the best fit for the data and assess the quality of each model.  

<img src="{{ site.url }}{{ site.baseurl }}/images/tamids/Figure3_12.jpg" alt="figure 3.12">  

Ultimately it comes down to AIC, AICc, and BIC values to determine which of the three ARIMA models is best. As it turns out, the naive model with seasonality activated is the best fit model for the data since its AIC, AICc, and BIC values are drastically lower than the other two models. In conclusion, our handpicked naive model
with seasonality out performed the model selected by auto.arima and is thus the best model for forecasting what the median daily fare in 2017 would look like for the average taxi in Chicago.  

# Additional Information
To find out more about the competition click [here](http://tamids.tamu.edu/2018-tamids-data-science-competition/).

To check out our code, visit our GitHub repository [here](https://github.com/kevintchou/TAMIDS-Competition).

This project was finished with under the collaboration with Jose Alfaro and Steven Broll. 
